%matplotlib inline
import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
from matplotlib.patches import PathPatch as Patch
import numpy as np
import pandas as pd
import time
import os
import seaborn as sns
from scipy import stats
import sklearn.preprocessing as skp
import sklearn.cluster as skc
from sklearn.mixture import GaussianMixture
from sklearn.cluster import KMeans
from sklearn.mixture import BayesianGaussianMixture
from sklearn.model_selection import train_test_split
from scipy import linalg
import itertools


from matplotlib.patches import Ellipse
color_iter = itertools.cycle(["navy", "c", "cornflowerblue", "gold", "darkorange","green","pink"])

def draw_ellipse(mean, covariance, alpha,color,ax=None):
    ax = plt.gca()
    v, w = linalg.eigh(covariance)
    v = 10.0 * np.sqrt(2.0) * np.sqrt(v)
    u = w[0] / linalg.norm(w[0])
        # as the DP will not use every component it has access to
        # unless it needs it, we shouldn't plot the redundant
        # components.
        #if not np.any(Y_ == i):
         #   continue
        #plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], 0.8, color=color)

    # Plot an ellipse to show the Gaussian component
    angle = np.arctan(u[1] / u[0])
    angle = 180.0 * angle / np.pi  # convert to degrees
    # Draw the Ellipse
    for nsig in range(1, 4):
        ax.add_patch(Ellipse(mean,  v[0], v[1],
                             angle, alpha=alpha,color=color))
   
    #ell = Ellipse(mean, v[0], v[1], angle=180.0 + angle, color=color)
    #ell.set_clip_box(splot.bbox)
    #ell.set_alpha(alpha)
    #ax.add_patch(ell)
    #ax.add_artist(ell)
   
        



outdir = './data/processed_data'


 # read data from the file and plot 
filename = os.path.join(outdir, f"cleanded_insurance_data.csv" )    
insurance_df = pd.read_csv(filename)


X = insurance_df.drop(['is_claim','max_torque','max_power'], axis=1)
y = insurance_df['is_claim']


X_train, X_test, y_train_true, y_test_true = train_test_split(X, y, test_size=0.2)





X_train.head()


###Decide n-cluster using Elbow Method
bic=[]
aic =[]
k_range = range(2,15)
for i in k_range:
    gmm_temp = GaussianMixture(n_components=i, n_init=10)
    gmm_temp.fit(X_train)
    bic.append(gmm_temp.bic(X_train))
    aic.append(gmm_temp.aic(X_train))

# save the data for future use
bic_filename = os.path.join(outdir, f"insurance_bic_scores.npy" )    
np.save(bic_filename, bic)

aic_filename = os.path.join(outdir, f"insurance_aic_scores.npy" )    
np.save(aic_filename, aic)


print(len(bic))


bic_filename = os.path.join(outdir, f"insurance_bic_scores.npy" )    
aic_filename = os.path.join(outdir, f"insurance_bic_scores.npy" )    
x_range = np.arange(2,15)
bic=np.load(bic_filename)
aic =np.load(aic_filename)


fig1 = plt.figure(figsize = (5,4))
plt.plot(x_range, bic, marker='o', label = "BIC Curve")
plt.plot(x_range, aic, marker='x', label = "AIC Curve")
plt.xlabel("Number of Clusters ")
plt.ylabel("AIC/BIC Values")
# for i, value in enumerate(wcss):
#     ax.text(i+1.05, value-0.005, round(value,1), fontsize=12, fontweight='bold')
    
#fig1.suptitle("Elbow Method");
plt.legend()
plt.show()


bgm = BayesianGaussianMixture(n_components=12, n_init=10, reg_covar=1e-2)
bgm.fit(X_train)
np.round(bgm.weights_, 2)


gmm = GaussianMixture(n_components=11, n_init=10)


gmm.fit(X_train)


predictions = gmm.predict(X_train)


labels = gmm.predict(X_train)


predictions.shape


X_train.shape


X_train['pred_prob'] = predictions
X_train['labels'] = labels


sns.pairplot(X_train, vars=["policy_tenure","age_of_car"], hue="pred_prob")


#size = 50 * predictions.max(1) ** 2  # square emphasizes differences
#plt.scatter(X_train["BMI"], X_train["GenHlth"], c=labels, cmap='viridis', s=size);
ax = plt.gca()
ax.scatter(X_train["policy_tenure"], X_train["age_of_car"], c=labels, s=40, cmap='viridis', zorder=2)
w_factor = 0.2 / gmm.weights_.max()
for mean, covar, w, color in zip(gmm.means_, gmm.covariances_, gmm.weights_, color_iter):
    alpha =w * w_factor 
    draw_ellipse(mean, covar, w, color,ax)

plt.show()


cluster_means = gmm.means_




plt.close('all')
plt.figure(1)
plt.clf()

colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')
for k, col in zip(range(n_clusters_), colors):
    class_members = predictions == k
    cluster_center = X[cluster_centers_indices[k]]
    plt.plot(X[class_members, 0], X[class_members, 1], col + '.')
    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,
             markeredgecolor='k', markersize=14)
    for x in X[class_members]:
        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)

plt.title('Estimated number of clusters: %d' % n_clusters_)
plt.show()


#size = 50 * predictions.max(1) ** 2  # square emphasizes differences
#plt.scatter(X_train["BMI"], X_train["GenHlth"], c=labels, cmap='viridis', s=size);
ax = plt.gca()
ax.scatter(X_train["make"], X_train["displacement"], c=labels, s=40, cmap='viridis', zorder=2)
w_factor = 0.2 / gmm.weights_.max()
for mean, covar, w, color in zip(gmm.means_, gmm.covariances_, gmm.weights_, color_iter):
    alpha =w * w_factor 
    draw_ellipse(mean, covar, w, color,ax)

plt.show()


gmm.converged_


X_train.columns


sns.boxplot(X_train, y='make',x='airbags')






