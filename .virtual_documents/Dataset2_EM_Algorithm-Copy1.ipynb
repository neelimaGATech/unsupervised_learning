%matplotlib inline
import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
from matplotlib.patches import PathPatch as Patch
import numpy as np
import pandas as pd
import time
import os
import seaborn as sns
from scipy import stats
import sklearn.preprocessing as skp
import sklearn.cluster as skc
from sklearn.mixture import GaussianMixture
from sklearn.cluster import KMeans
from sklearn.mixture import BayesianGaussianMixture
from sklearn.model_selection import train_test_split
from scipy import linalg


color_iter = itertools.cycle(["navy", "c", "cornflowerblue", "gold", "darkorange"])

def plot_results(X, Y_, means, covariances, index, title):
    splot = plt.subplot(2, 1, 1 + index)
    for i, (mean, covar, color) in enumerate(zip(means, covariances, color_iter)):
        v, w = linalg.eigh(covar)
        v = 2.0 * np.sqrt(2.0) * np.sqrt(v)
        u = w[0] / linalg.norm(w[0])
        # as the DP will not use every component it has access to
        # unless it needs it, we shouldn't plot the redundant
        # components.
        if not np.any(Y_ == i):
            continue
        plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], 0.8, color=color)

        # Plot an ellipse to show the Gaussian component
        angle = np.arctan(u[1] / u[0])
        angle = 180.0 * angle / np.pi  # convert to degrees
        ell = mpl.patches.Ellipse(mean, v[0], v[1], angle=180.0 + angle, color=color)
        ell.set_clip_box(splot.bbox)
        ell.set_alpha(0.5)
        splot.add_artist(ell)

    plt.xlim(-9.0, 5.0)
    plt.ylim(-3.0, 6.0)
    plt.xticks(())
    plt.yticks(())
    plt.title(title)


outdir = './data/processed_data'


 # read data from the file and plot 
filename = os.path.join(outdir, f"cleanded_diabetes_data.csv" )    
diabetes_df = pd.read_csv(filename)


# drop Diabetes_012 column from dataset to get input matrix X
X = diabetes_df.drop('Diabetes_012', axis=1)
# Extract Diabetes_012 column to generate output vector y
y = diabetes_df['Diabetes_012']


X_train, X_test, y_train_true, y_test_true = train_test_split(X, y, test_size=0.2)





###Decide n-cluster using Elbow Method
bic=[]
aic =[]
k_range = range(2,10)
for i in k_range:
    gmm_temp = GaussianMixture(n_components=i, n_init=10)
    gmm_temp.fit(X_train)
    bic.append(gmm_temp.bic(X_train))
    aic.append(gmm_temp.aic(X_train))

# save the data for future use
bic_filename = os.path.join(outdir, f"bic_scores2_10.npy" )    
np.save(bic_filename, bic)

aic_filename = os.path.join(outdir, f"aic_scores2_10.npy" )    
np.save(aic_filename, aic)


print(len(bic))


bic_filename = os.path.join(outdir, f"bic_scores2_10.npy" )    
aic_filename = os.path.join(outdir, f"bic_scores2_10.npy" )    
x_range = np.arange(2,8)
bic=np.load(bic_filename)
aic =np.load(aic_filename)


fig1 = plt.figure(figsize = (5,4))
plt.plot(x_range, bic, marker='o', label = "BIC Curve")
plt.plot(x_range, aic, marker='x', label = "AIC Curve")
plt.xlabel("Number of Clusters ")
plt.ylabel("AIC/BIC Values")
# for i, value in enumerate(wcss):
#     ax.text(i+1.05, value-0.005, round(value,1), fontsize=12, fontweight='bold')
    
#fig1.suptitle("Elbow Method");
plt.legend()
plt.show()


bgm = BayesianGaussianMixture(n_components=8, n_init=10)
bgm.fit(X_train)
np.round(bgm.weights_, 2)


gmm = GaussianMixture(n_components=3, n_init=10)


gmm.fit(X_train)


plot_results(X_train, gmm.predict(X_train), gmm.means_, gmm.covariances_, 0, "Gaussian Mixture")


labels = gmm.predict(X_train)





gmm.converged_


X_train.columns


sns.boxplot(X_train, y='BMI',x=labels)


probs = gmm.predict_proba(X_train)
print(probs[:5].round(3))


size = 50 * probs.max(1) ** 2  # square emphasizes differences
plt.scatter(X_train.iloc[:, 3], X_train.iloc[:, 20], c=labels, cmap='viridis', s=size);


size


gmm.means_






